# Kasparro Backend & ETL System

A production-grade backend ETL system for cryptocurrency data with multi-source ingestion, **identity unification**, RESTful API, and cloud deployment.

## ğŸŒ Live Demo

**API:** https://kasparro-api-18sg.onrender.com

| Endpoint | URL |
|----------|-----|
| Health | https://kasparro-api-18sg.onrender.com/health |
| Data | https://kasparro-api-18sg.onrender.com/data?limit=10 |
| Stats | https://kasparro-api-18sg.onrender.com/stats |
| Docs | https://kasparro-api-18sg.onrender.com/docs |

## ğŸš€ Quick Start

```bash
# Clone repository
git clone https://github.com/UtkarshJi/kasparro-backend-utkarsh-sharma.git
cd kasparro-backend-utkarsh-sharma

# Start all services
make up

# Run tests
make test

# View logs
make logs

# Stop services
make down
```

## ğŸ“‹ Features

### Data Sources
- **CoinPaprika API** - Cryptocurrency ticker data (2000+ coins)
- **CoinGecko API** - Market data with prices and rankings
- **CSV** - Product data ingestion

### ğŸ”— Identity Unification (NEW)
Same cryptocurrency from different sources (CoinPaprika + CoinGecko) is **unified into a single record** using symbol-based canonical IDs:
- Bitcoin from both sources â†’ `canonical_id='btc'` â†’ **1 unified record**
- No duplicate entries for the same coin
- Cross-source data merging

### P0 - Foundation âœ…
- Multi-source ETL (CoinPaprika + CoinGecko + CSV)
- PostgreSQL storage (raw + normalized)
- Pydantic validation with type cleaning
- Incremental ingestion with checkpoints
- RESTful API with pagination & filtering
- Health endpoint with DB/ETL status

### P1 - Growth âœ…
- Third data source (CSV)
- Checkpoint-based resume on failure
- Idempotent writes (upserts)
- `/stats` endpoint with analytics
- Comprehensive test suite (76 tests)

### P2 - Differentiator âœ…
- **Identity unification** across data sources
- Schema drift detection with fuzzy matching
- Failure recovery with checkpoints
- Rate limiting with exponential backoff
- Prometheus metrics (`/metrics`)
- Structured JSON logging
- Run comparison endpoints

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CoinPaprika     â”‚  â”‚    CoinGecko      â”‚  â”‚       CSV         â”‚
â”‚   (API 1)         â”‚  â”‚    (API 2)        â”‚  â”‚    (Products)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚                      â”‚                      â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚    ETL Pipeline       â”‚
                     â”‚  â€¢ Fetch with retry   â”‚
                     â”‚  â€¢ Validate (Pydantic)â”‚
                     â”‚  â€¢ Transform          â”‚
                     â”‚  â€¢ Identity Resolver  â”‚ â† Generates canonical_id
                     â”‚  â€¢ Upsert by canon_id â”‚ â† Merges same coins
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚      PostgreSQL       â”‚
                     â”‚  â€¢ raw_api_data       â”‚
                     â”‚  â€¢ raw_csv_data       â”‚
                     â”‚  â€¢ unified_data       â”‚ â† canonical_id unique
                     â”‚  â€¢ etl_checkpoints    â”‚
                     â”‚  â€¢ etl_runs           â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚       FastAPI         â”‚
                     â”‚  GET /data            â”‚
                     â”‚  GET /health          â”‚
                     â”‚  GET /stats           â”‚
                     â”‚  GET /metrics         â”‚
                     â”‚  POST /api/etl/triggerâ”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¡ API Endpoints

### GET /health
Health check with DB and ETL status.
```bash
curl https://kasparro-api-18sg.onrender.com/health
```

### GET /data
Fetch cryptocurrency data with pagination and filtering.
```bash
curl "https://kasparro-api-18sg.onrender.com/data?limit=10&source=coinpaprika"
```

### GET /stats
ETL run statistics and per-source breakdowns.
```bash
curl https://kasparro-api-18sg.onrender.com/stats
```

### GET /metrics
Prometheus-format metrics.
```bash
curl https://kasparro-api-18sg.onrender.com/metrics
```

### POST /api/etl/trigger
Manually trigger ETL run.
```bash
curl -X POST https://kasparro-api-18sg.onrender.com/api/etl/trigger
```

## ğŸ”§ Configuration

Copy `.env.example` to `.env`:
```bash
cp .env.example .env
```

### Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `DATABASE_URL` | PostgreSQL connection string | Auto-converted for asyncpg |
| `COINPAPRIKA_API_KEY` | Optional API key | Not required |
| `ETL_INTERVAL_MINUTES` | ETL run frequency | `5` |
| `ETL_BATCH_SIZE` | Records per batch | `50` |
| `LOG_LEVEL` | Logging level | `INFO` |

## ğŸ§ª Testing

```bash
# Run all tests (76 tests)
make test

# Run with coverage
make test-coverage

# Run specific test file
docker compose run --rm api pytest tests/test_etl/ -v
```

## ğŸ“ Project Structure

```
kasparro_backend/
â”œâ”€â”€ api/                    # FastAPI application
â”‚   â”œâ”€â”€ routes/             # API endpoints
â”‚   â””â”€â”€ dependencies.py     # Shared dependencies
â”œâ”€â”€ ingestion/              # ETL pipeline
â”‚   â”œâ”€â”€ sources/            # Data source connectors
â”‚   â”‚   â”œâ”€â”€ coinpaprika_source.py
â”‚   â”‚   â”œâ”€â”€ coingecko_source.py
â”‚   â”‚   â””â”€â”€ csv_source.py
â”‚   â””â”€â”€ pipeline.py         # Orchestration
â”œâ”€â”€ services/               # Business logic
â”‚   â”œâ”€â”€ identity_resolver.py # Cross-source ID unification
â”‚   â”œâ”€â”€ rate_limiter.py     # Token bucket
â”‚   â””â”€â”€ schema_drift.py     # Drift detection
â”œâ”€â”€ schemas/                # Pydantic models
â”œâ”€â”€ core/                   # Configuration
â”œâ”€â”€ tests/                  # Test suite (76 tests)
â””â”€â”€ data/                   # Sample CSV files
```

## ğŸš¢ Deployment

### Local Docker
```bash
make up
# API available at http://localhost:8000
```

### Cloud (Render)
The application is deployed on Render with:
- PostgreSQL database
- Docker web service
- Auto-deploy from GitHub
- Scheduled ETL every 5 minutes
- Automatic schema migration

## ğŸ“Š Tech Stack

- **Framework:** FastAPI
- **Database:** PostgreSQL + SQLAlchemy (async)
- **ETL:** Custom pipeline with APScheduler
- **Identity Unification:** Symbol-based canonical IDs
- **Validation:** Pydantic
- **Testing:** Pytest (76 tests)
- **Logging:** Structlog (JSON format)
- **Metrics:** Prometheus
- **Container:** Docker + Docker Compose

## ğŸ‘¤ Author

**Utkarsh Sharma**

## ğŸ“„ License

MIT

